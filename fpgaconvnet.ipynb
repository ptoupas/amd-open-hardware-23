{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c736a15f-acbc-40fb-9afc-4999dbb8617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/share/pynq-venv/lib/python3.10/site-packages (4.64.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==0.14.1 (from versions: 1.12.0, 1.12.1, 1.13.1, 1.14.0, 1.14.1, 1.15.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==0.14.1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pynq\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from pynq import MMIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f99bdfd-3083-49b4-ba1d-6941522a4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../common\"))\n",
    "\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "import pynq\n",
    "import dac_sdc\n",
    "from IPython.display import display\n",
    "\n",
    "team_name = 'fpgaconvnet'\n",
    "dac_sdc.BATCH_SIZE = 64\n",
    "team = dac_sdc.Team(team_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2633c11a-4d88-44e1-9448-57d32f2b854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pynq\n",
    "\n",
    "@dataclass\n",
    "class Partition:\n",
    "    index: int \n",
    "    n_dma: int\n",
    "    input_sizes: List[tuple[int,int,int]]\n",
    "    output_sizes: List[tuple[int,int,int]]\n",
    "    output_streams: List[int]\n",
    "    input_bp: List[int]\n",
    "    output_bp: List[int]\n",
    "    batch_size: int = 64\n",
    "    \n",
    "    def __post_init__(self):\n",
    "\n",
    "        # create an overlay\n",
    "        self.overlay = pynq.Overlay(f\"bitstreams/p{self.index}.bit\")\n",
    "        \n",
    "        # get all DMA\n",
    "        self.dma = [ getattr(self.overlay, f\"dma_{i}\")\\\n",
    "                        for i in range(self.n_dma) ]\n",
    "        \n",
    "        # initialise partition register file\n",
    "        self.baseaddr = 0xA0020000\n",
    "        self.regfile = MMIO(self.baseaddr, 0x1000)\n",
    "        \n",
    "        # allocate input buffers\n",
    "        self.input_buffers = [ pynq.allocate(\n",
    "            shape=(self.batch_size*math.prod(s)), \n",
    "            dtype=np.int16) for s in self.input_sizes ]\n",
    "        \n",
    "        # allocate output buffers\n",
    "        self.output_buffers = [ pynq.allocate(\n",
    "            shape=(self.batch_size*math.prod(s)), \n",
    "            dtype=np.int16) for s in self.output_sizes ]\n",
    "\n",
    "        # empty set of weight buffers\n",
    "        self.weight_buffers = {}\n",
    "        \n",
    "        # setup the hardwarte\n",
    "        self.setup_hardware()\n",
    "    \n",
    "    def update_input_buffers(self, batch_size):\n",
    "        # allocate input buffers\n",
    "        self.input_buffers = [ pynq.allocate(\n",
    "            shape=(batch_size*math.prod(s)), \n",
    "            dtype=np.int16) for s in self.input_sizes ]\n",
    "\n",
    "    def update_output_buffers(self, batch_size):\n",
    "        # allocate output buffers\n",
    "        self.output_buffers = [ pynq.allocate(\n",
    "            shape=(batch_size*math.prod(s)), \n",
    "            dtype=np.int16) for s in self.output_sizes ]\n",
    "        \n",
    "    def setup_hardware(self):\n",
    "        # initialise the hardware\n",
    "        self.regfile.write(0x0, 0)\n",
    "        for (i, (width, size)) in enumerate(zip(self.output_streams, self.output_sizes)):\n",
    "            self.regfile.write(0x8+i*4, \n",
    "                self.batch_size*math.prod(size)//width)\n",
    "        self.start_hardware() \n",
    "        \n",
    "    def reset_hardware(self):\n",
    "        self.regfile.write(0x0, 0x2)\n",
    "        self.regfile.write(0x0, 0x0)\n",
    "        \n",
    "    def start_hardware(self):\n",
    "        self.regfile.write(0x0, 0x4)\n",
    "        \n",
    "    def stop_hardware(self):\n",
    "        self.regfile.write(0x0, 0x0)\n",
    "        \n",
    "    def allocate_weights(self, index: int, weights_filepath: str):\n",
    "        \n",
    "        # load the weights into a numpy array\n",
    "        with open(weights_filepath, \"r\") as f:\n",
    "             weights = np.array([int(x, base=16) \\\n",
    "                        for x in f.readlines() ], dtype=np.uint32)\n",
    "\n",
    "        # allocate a pynq buffer for the weights\n",
    "        self.weight_buffers[index] = pynq.allocate(\n",
    "                shape=weights.shape, dtype=np.uint32)\n",
    "        \n",
    "        # get the values of weights\n",
    "        self.weight_buffers[index][:] = weights\n",
    "\n",
    "    def reload_weights(self, index: int):\n",
    "        \n",
    "        # set to update mode\n",
    "        self.regfile.write(0x0, 0x1)  \n",
    "             \n",
    "        # set the weight index\n",
    "        self.regfile.write(0x4, index)  \n",
    "\n",
    "        # transfer the weights\n",
    "        self.dma[1].sendchannel.transfer(self.weight_buffers[index])\n",
    "        \n",
    "        # wait for transfer to finish\n",
    "        self.dma[1].sendchannel.wait()\n",
    "        \n",
    "        # end update mode\n",
    "        self.regfile.write(0x0, 0x0)   \n",
    "        \n",
    "        self.reset_hardware()\n",
    "        \n",
    "        # set the weight index somewhere else\n",
    "        self.regfile.write(0x4, 0xFFFF)  \n",
    "        \n",
    "        self.start_hardware()\n",
    "        \n",
    "    def download(self):\n",
    "        \n",
    "        # download the bitstream\n",
    "        self.overlay.download()\n",
    "        \n",
    "        # setup the hardwarte\n",
    "        self.setup_hardware()\n",
    "        \n",
    "    def send_dma(self, index: int):\n",
    "        self.start_hardware()\n",
    "        self.dma[index].sendchannel.transfer(self.input_buffers[index])\n",
    "    \n",
    "    def recv_dma(self, index: int):\n",
    "        self.dma[index].recvchannel.transfer(self.output_buffers[index])\n",
    "    \n",
    "    def wait_dma(self, index: int):\n",
    "         \n",
    "        # wait to receive\n",
    "        try:\n",
    "            self.dma[index].recvchannel.wait() \n",
    "        except:\n",
    "            print(\"WARNING: recv channel finished\")\n",
    "            \n",
    "        # wait to send\n",
    "        try:\n",
    "            self.dma[index].sendchannel.wait()\n",
    "        except:\n",
    "            print(\"WARNING: send channel finished\")         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca22af85-cbca-4a56-b7db-533077342bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p0 = Partition(0, 2, [[320, 320, 3]], [[40, 40, 64]], [4], [13], [11], batch_size=dac_sdc.BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e2f335-a180-4f81-948a-233ae11b4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def box_iou_batch(boxes_a: np.ndarray, boxes_b: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    def box_area(box):\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area_a = box_area(boxes_a.T)\n",
    "    area_b = box_area(boxes_b.T)\n",
    "\n",
    "    top_left = np.maximum(boxes_a[:, None, :2], boxes_b[:, :2])\n",
    "    bottom_right = np.minimum(boxes_a[:, None, 2:], boxes_b[:, 2:])\n",
    "\n",
    "    area_inter = np.prod(np.clip(bottom_right - top_left, a_min=0, a_max=None), 2)\n",
    "\n",
    "    return area_inter / (area_a[:, None] + area_b - area_inter)\n",
    "\n",
    "def non_max_suppression(predictions: np.ndarray, scores: np.ndarray, \n",
    "        categories: np.ndarray, iou_threshold: float = 0.5) -> np.ndarray:\n",
    "    rows, columns = predictions.shape\n",
    "\n",
    "    sort_index = np.flip(scores.argsort())\n",
    "\n",
    "    ious = box_iou_batch(predictions, predictions)\n",
    "    ious = ious - np.eye(rows)\n",
    "\n",
    "    keep = np.ones(rows, dtype=bool)\n",
    "\n",
    "    for index, (iou, category) in enumerate(zip(ious, categories)):\n",
    "        if not keep[index]:\n",
    "            continue\n",
    "\n",
    "        condition = (iou > iou_threshold) & (categories == category)\n",
    "        keep = keep & ~condition\n",
    "\n",
    "    return keep[sort_index.argsort()]\n",
    "\n",
    "def yolo_nms(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.5,\n",
    "        classes=None,\n",
    "        agnostic=False,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nm=0,  # number of masks\n",
    "):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) array per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    # t = time.time()\n",
    "    mi = 5 + nc  # mask start index\n",
    "    output = [np.zeros((0, 6 + nm))] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = np.zeros((len(lb), nc + nm + 5), device=x.device)\n",
    "            v[:, :4] = lb[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = np.concatenate((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "\n",
    "        mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = np.concatenate((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else:  # best class only\n",
    "            conf = x[:, 5:mi].max(axis=1, keepdims=True)\n",
    "            j = x[:, 5:mi].argmax(axis=1).reshape(-1, 1)\n",
    "            x = np.concatenate((box, conf, j.astype(np.float32), mask), 1)[conf.reshape(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == np.array(classes)).any(1)]\n",
    "\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "\n",
    "        x = x[x[:, 4].argsort()[::-1][:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores, categories = x[:, :4] + c, x[:, 4], x[:, 5]  # boxes (offset by class), scores\n",
    "\n",
    "        i = non_max_suppression(boxes, scores, categories, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "\n",
    "        output[xi] = x[i]\n",
    "\n",
    "        # if (time.time() - t) > time_limit:\n",
    "        #     print(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "        #     break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ac9fb1-7dde-4924-a4c7-e58dcd2e3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=False, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_NEAREST)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def img_preprocess(img, imgsz, stride):\n",
    "    im, _, _ = letterbox(img, imgsz, stride)  # padded resiz\n",
    "    # im = im.transpose((2, 0, 1))  # HWC to CHW\n",
    "    im = im.astype(np.float32)\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72448a5c-a468-4785-96d2-06f149c3a6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc8f5f2-9de2-4a33-a8fa-dfb856b783e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime==1.14.1 in /usr/local/share/pynq-venv/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from onnxruntime==1.14.1) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from onnxruntime==1.14.1) (23.5.26)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from onnxruntime==1.14.1) (1.24.3)\n",
      "Requirement already satisfied: packaging in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from onnxruntime==1.14.1) (23.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from onnxruntime==1.14.1) (4.23.3)\n",
      "Requirement already satisfied: sympy in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from onnxruntime==1.14.1) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime==1.14.1) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/share/pynq-venv/lib/python3.10/site-packages (from sympy->onnxruntime==1.14.1) (1.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/share/pynq-venv/lib/python3.10/site-packages (4.64.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime==1.14.1\n",
    "!pip install tqdm\n",
    "\n",
    "import onnxruntime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import onnx\n",
    "# from onnxsim import simplify\n",
    "# from onnxruntime.tools.onnx_model_utils import fix_output_shapes, make_input_shape_fixed\n",
    "\n",
    "\n",
    "def load_model(model_path, gpu=False):\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if gpu else ['CPUExecutionProvider']\n",
    "    session = onnxruntime.InferenceSession(model_path, providers=providers)\n",
    "    output_names = [x.name for x in session.get_outputs()]\n",
    "    meta = session.get_modelmeta().custom_metadata_map  # metadata\n",
    "    if 'stride' in meta:\n",
    "        stride, names = int(meta['stride']), eval(meta['names'])\n",
    "    else:\n",
    "        stride = 32\n",
    "        names = {0: 'motor vehicle', 1: 'non-motor vehicle', 2: 'pedestrian', 3: 'red light', 4: 'yellow light', 5: 'green light', 6: 'off light'}\n",
    "    return stride, names, session, output_names\n",
    "\n",
    "def model_inf(img, session, output_names):\n",
    "    # inference\n",
    "    outputs = session.run(output_names, {session.get_inputs()[0].name: img})\n",
    "\n",
    "    return outputs[0] if len(outputs) == 1 else [x for x in outputs]\n",
    "\n",
    "img_size = 320\n",
    "floating_point_precision = 32\n",
    "\n",
    "# fix the input shape of the model\n",
    "onnx_model_path = f'onnx_models/yolov5n-imgsz-{img_size}-hs-trained_dynamic_batch_part_1.onnx'\n",
    "\n",
    "# onnx_model_opt_path = f'onnx_models/yolov5n-imgsz-{img_size}-hs-trained_dynamic_batch_part_1_opt.onnx'\n",
    "# model = onnx.load(onnx_model_path)\n",
    "# make_input_shape_fixed(model.graph, \", args.input_shape)\n",
    "# fix_output_shapes(model)\n",
    "# model, _ = simplify(model)\n",
    "\n",
    "# onnx.save(model, onnx_model_opt_path)\n",
    "\n",
    "stride, names, session, output_names = load_model(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10686022-8c3f-4e7f-8609-876f7d596a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = os.listdir('images')\n",
    "# rgb_imgs = []\n",
    "# for img in images:\n",
    "#     if img.endswith('jpg'):\n",
    "#         img_path = os.path.join('images', img)\n",
    "#         bgr_img = cv2.imread(img_path)\n",
    "#         assert bgr_img is not None, f'Image Not Found {img_path}'\n",
    "#         rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "#         rgb_imgs.append((img_path, rgb_img))\n",
    "# team.img_list = team.img_list[:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b21bf0a-07fe-4e4f-84dd-81cd5070deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, shape):\n",
    "    # Clip boxes (xyxy) to image shape (height, width)\n",
    "    boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "    boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])  # y1, y2\n",
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    # Rescale boxes (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[..., [0, 2]] -= pad[0]  # x padding\n",
    "    boxes[..., [1, 3]] -= pad[1]  # y padding\n",
    "    boxes[..., :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d1556cd-32e4-4a07-a841-d3e5d977704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(data, binary_point = 8):\n",
    "    data = data * (2**binary_point)\n",
    "    return data.astype(np.int16)\n",
    "\n",
    "def format_output(data, binary_point = 8, transpose = True):\n",
    "    data = data.astype(np.float32) / float(2**binary_point)\n",
    "    if transpose:\n",
    "        return np.moveaxis(data, -1, 1)\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dbdef4a-a7d8-40d2-906c-2f0ce463c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (dac_sdc.BATCH_SIZE, 320, 320, 3)\n",
    "output_shape = (dac_sdc.BATCH_SIZE, 40, 40, 64)\n",
    "\n",
    "visualize = False\n",
    "conf_thres = 0.25\n",
    "iou_thres = 0.5\n",
    "classes = None\n",
    "imgsz = [img_size]\n",
    "imgsz *= 2 if len(imgsz) == 1 else 1\n",
    "    \n",
    "visualise_path = \"images_result_p0_only/\"\n",
    "\n",
    "def my_callback(rgb_imgs):\n",
    "    object_locations_by_image = {}\n",
    "    \n",
    "    batch_img = []\n",
    "    # batch_rgb_imgs = []\n",
    "    batch_vis_imgs = []\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    for path, img in rgb_imgs:\n",
    "        \n",
    "        # if visualize:\n",
    "        #     im0 = img.copy()\n",
    "        #     im0 = cv2.cvtColor(im0, cv2.COLOR_RGB2BGR)\n",
    "        #     batch_vis_imgs.append(im0)\n",
    "        # else:\n",
    "        batch_vis_imgs.append((path, img.shape))\n",
    "\n",
    "        img = img_preprocess(img, imgsz, stride)\n",
    "        \n",
    "        # batch_rgb_imgs.append((path, img.shape))   \n",
    "        batch_img.append(img)\n",
    "            \n",
    "\n",
    "    batch_img = np.vstack(batch_img)\n",
    "    # print(f\"Batching images took {time.time()-start_time:.5f} sec\")\n",
    "    \n",
    "    curr_batch_size = batch_img.shape[0]\n",
    "        \n",
    "    if curr_batch_size < dac_sdc.BATCH_SIZE:\n",
    "        p0.update_input_buffers(batch_size = curr_batch_size)\n",
    "        p0.update_output_buffers(batch_size = curr_batch_size)\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    # Put resized image into DMA\n",
    "    p0.input_buffers[0][:] = format_input(batch_img.flatten(), binary_point=p0.input_bp[0])\n",
    "    # print(f\"Format and copy data to input buffers took {time.time()-start_time:.5f} sec\")\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    p0.send_dma(0)\n",
    "    p0.recv_dma(0)\n",
    "    p0.wait_dma(0)\n",
    "    # print(f\"DMA transfer complete in {time.time()-start_time:.5f} sec\")\n",
    "\n",
    "    # start_time = time.time()\n",
    "    if curr_batch_size < dac_sdc.BATCH_SIZE:\n",
    "        out_buffer_float = format_output(np.reshape(copy.deepcopy(p0.output_buffers[0]), (curr_batch_size, *output_shape[1:])), binary_point=p0.output_bp[0])\n",
    "    else:\n",
    "        out_buffer_float = format_output(np.reshape(copy.deepcopy(p0.output_buffers[0]), output_shape), binary_point=p0.output_bp[0])\n",
    "    # out_buffer_float = (np.moveaxis(np.reshape(out_buffer, output_shape), -1, 1) / 256).astype(np.float32)\n",
    "    # print(f\"Format and copy data to output buffers took {time.time()-start_time:.5f} sec\")\n",
    "    \n",
    "    # call onnx model inference\n",
    "    # onnx_inference_start_time = time.perf_counter()\n",
    "    batch_pred = model_inf(out_buffer_float, session, output_names)\n",
    "    # batch_pred = model_inf(batch_img, session, output_names)\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    batch_nms_pred = yolo_nms(batch_pred, conf_thres, iou_thres, classes)\n",
    "    # print(f\"NMS took {time.time() - start_time:.5f} sec\")\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    for pred, (path, im0) in zip(batch_nms_pred, batch_vis_imgs):\n",
    "        # if visualize:\n",
    "        #     pred[:, :4] = scale_boxes((320, 320), pred[:, :4], im0.shape).round()\n",
    "        # else:\n",
    "        pred[:, :4] = scale_boxes((320, 320), pred[:, :4], im0).round()\n",
    "\n",
    "        object_locations = []\n",
    "        for p in pred:\n",
    "            object_locations.append({\"type\":int(p[5]) + 1, \"x\":int(p[0]), \"y\":int(p[1]), \"width\":int(p[2])-int(p[0]), \"height\":int(p[3])-int(p[1])})\n",
    "#             if visualize:\n",
    "#                 cv2.rectangle(im0, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), (0, 255, 0), 2)\n",
    "#                 cv2.putText(im0, names[int(p[5])], (int(p[0]), int(p[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "#         if visualize:\n",
    "#             res_img_path = os.path.join(visualise_path, str(path).split('/')[-1])\n",
    "#             cv2.imwrite(res_img_path, im0)\n",
    "\n",
    "        # Save to dictionary by image filename\n",
    "        object_locations_by_image[path.name] = object_locations\n",
    "    # print(f\"Writing results to dictionary took {time.time() - start_time:.5f} sec\")\n",
    "    \n",
    "    return object_locations_by_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07700b7f-bb38-4a67-96e0-ec30fb077978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 starting. 64 images.\n",
      "Batch 1 done. Runtime = 6.324365139007568 seconds.\n",
      "Batch 2 starting. 64 images.\n",
      "Batch 2 done. Runtime = 6.199946880340576 seconds.\n",
      "Batch 3 starting. 64 images.\n",
      "Batch 3 done. Runtime = 6.071592092514038 seconds.\n",
      "Batch 4 starting. 64 images.\n",
      "Batch 4 done. Runtime = 6.160435914993286 seconds.\n",
      "Batch 5 starting. 64 images.\n",
      "Batch 5 done. Runtime = 6.030292987823486 seconds.\n",
      "Batch 6 starting. 64 images.\n",
      "Batch 6 done. Runtime = 6.057433843612671 seconds.\n",
      "Batch 7 starting. 64 images.\n",
      "Batch 7 done. Runtime = 6.121445894241333 seconds.\n",
      "Batch 8 starting. 64 images.\n",
      "Batch 8 done. Runtime = 6.090564966201782 seconds.\n",
      "Batch 9 starting. 64 images.\n",
      "Batch 9 done. Runtime = 6.111550807952881 seconds.\n",
      "Batch 10 starting. 64 images.\n",
      "Batch 10 done. Runtime = 6.091575622558594 seconds.\n",
      "Batch 11 starting. 64 images.\n",
      "Batch 11 done. Runtime = 6.100207090377808 seconds.\n",
      "Batch 12 starting. 64 images.\n",
      "Batch 12 done. Runtime = 6.11053991317749 seconds.\n",
      "Batch 13 starting. 64 images.\n",
      "Batch 13 done. Runtime = 6.172656059265137 seconds.\n",
      "Batch 14 starting. 64 images.\n",
      "Batch 14 done. Runtime = 6.217107534408569 seconds.\n",
      "Batch 15 starting. 61 images.\n",
      "WARNING: recv channel finished\n",
      "WARNING: send channel finished\n",
      "Batch 15 done. Runtime = 6.032865047454834 seconds.\n",
      "Done all batches. Total runtime = 91.89257979393005 seconds. Total energy = 0 J.\n",
      "Savings results to JSON...\n",
      "JSON results written successfully.\n"
     ]
    }
   ],
   "source": [
    "team.run(my_callback, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78de5c14-1d39-498b-b5fd-38aa6bc497af",
   "metadata": {},
   "outputs": [],
   "source": [
    "team.reset_batch_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ccdbe51-56f1-456b-ab08-38827cd461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p0.update_input_buffers(batch_size = dac_sdc.BATCH_SIZE)\n",
    "# p0.update_output_buffers(batch_size = dac_sdc.BATCH_SIZE)\n",
    "\n",
    "del p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bf591-5703-4ca4-8504-5ee87505d04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7296f8c-e5c8-4464-a0a4-c8864532626d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
